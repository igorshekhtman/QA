https://reports.apixio.com/html/index.htm

===========================
ADD SEQ FILE CLOSE - LANCE:
===========================
{"time":"1391047563506","level":"EVENT","source":"production.docreceiver","hostname":"ip-10-199-9-13","datestamp":"2014-01-30T02:06:03.507Z","jvm":{"threads":{"count":"158"},"thread":"201","processors":{"count":"4"},"memory":{"total":{"bytes":"1557135360","count":"1"},"free":{"bytes":"887058064","count":"1"},"max":{"bytes":"3469213696","count":"1"}}},"loggerName":"com.apixio.docreceiver.service.SequenceFileServiceFile","seqfile":{"file":{"close":{"numkeys":"440","bytes":"66988082","count":"1",,"status":"success","directory":"hdfs://10.199.6.149:8020/user/apxqueue/receiver/10000262_backload/dir_1391047368239","filename":"hdfs://10.199.6.149:8020/user/apxqueue/receiver/10000262_backload/dir_1391047368239/file_1391047368239.apxseq","millis":"13"}}}}



=====================================================================================================================
Doc-Receiver Performance test:
====================================== UPLOAD =======================================================================

get_json_object(line, '$.upload.document.batchid')

get_json_object(line, '$.upload.document.bytes'):3913 (size of encrypted zipped document+cataqlog)
get_json_object(line, '$.upload.document.save.millis'):0
get_json_object(line, '$.upload.document.package.millis'):1
get_json_object(line, '$.upload.document.upstream.millis'):32
get_json_object(line, '$.upload.document.encrypt.millis'):2

get_json_object(line, '$.upload.document.file.bytes'):81890 (size of decrypted document)
get_json_object(line, '$.upload.document.file.millis'):0

get_json_object(line, '$.upload.document.serialize.bytes'):83131
get_json_object(line, '$.upload.document.serialize.millis'):0

get_json_object(line, '$.upload.document.hash.millis'):5

get_json_object(line, '$.upload.document.catalog.bytes'):998 (size of catalog file)
get_json_object(line, '$.upload.document.catalog.millis'):0

get_json_object(line, '$.upload.document.http.millis'):3

get_json_object(line, '$.upload.document.archive.millis'):122

get_json_object(line, '$.upload.document.seqfile.millis'):122

====================================== ARCHIVE =====================================================================

get_json_object(line, '$.archive.afs.batchid')

get_json_object(line, '$.archive.afs.bytes'): 3913
get_json_object(line, '$.archive.afs.millis'): 122

====================================== SEQ. FILE ===================================================================

get_json_object(line, '$.seqfile.file.document.batchid')


get_json_object(line, '$.seqfile.file.document.bytes'):3913
get_json_object(line, '$.seqfile.file.document.??????')

get_json_object(line, '$.seqfile.file.add.bytes'):4248
get_json_object(line, '$.seqfile.file.add.millis'):1

====================================== SUBMIT =======================================================================

get_json_object(line, '$.submit.post.batchid')

get_json_object(line, '$.submit.post.bytes'):4248
get_json_object(line, '$.submit.post.millis'):62

=====================================================================================================================

https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF
[2:33:33 PM] Lance N (Apixio): stddev(cast(json_object(...) as INTEGER))

=============================================================================================================
Blocking IP Address for Stagin DR Environmental Components test:
=============================================================================================================

I added these three critical services that must also be tested. Pretty much everything can be tested by just blocking access to the right IP. I was able to trivially achieve this on linux with the following command:
sudo iptables -A INPUT -s 74.125.239.101 -j DROP

and then remove the rule with:
sudo iptables -D INPUT -s 74.125.239.101 -j DROP

Just replace the IP address here with the one you want to block. -A "adds" the rule and -D "deletes" it:

add: 
sudo iptables -A INPUT -s 74.125.239.101 -j DROP
remove:
sudo iptables -D INPUT -s 74.125.239.101 -j DROP

outgoing is -d not -s (Lance)
This is how to cut off access to the API server from supload:

iptables -A OUTPUT -d 10.196.82.47 -j DROP


This means:
-A       add a rule for sending packets
-d 10.196.82.47   about sending packets to this IP
-j DROP   drop packets

To remove the rule use -D instead of -A

I got the address by running this inside Amazon:
ping supapi.apixio.com

But you have the IPs for everything else. Always use the inside IP address.

You can remove all of the rules with:
iptables -F

---------------------------------------------- Lance ---------------------------------------------------------------
ok, got them:

ssh -i /mnt/automation/.secrets/staging.pem supload2.apixio.com iptables -L

lists the iptables rule list
ssh -i /mnt/automation/.secrets/staging.pem supload2.apixio.com iptables -A OUTPUT -d 10.198.43.98 -j DROP

ssh -i /mnt/automation/.secrets/staging.pem supload2.apixio.com iptables -A OUTPUT -d 10.198.43.98 -j DROP

ssh -i /mnt/automation/.secrets/staging.pem supload2.apixio.com iptables -F

removes all of the iptables rules. do this before every test.

ssh -i /mnt/automation/.secrets/supload2.pem 10.199.16.28 iptables -A OUTPUT -d 10.198.43.98 -j DROP

ssh -i /mnt/automation/.secrets/supload2.pem 10.199.16.28 iptables -L


-L shows the list of rules
-A       add a rule for sending packets
-D       remove the rule from list
-F	 removes all of the iptables rules
-d 10.196.82.47   about sending packets to this IP
-j DROP   drop packets



Hive        is 184.169.209.24 outside    /10.196.47.205 inside
Fluent      is 54.219.88.28 outside      /10.222.103.158 inside
Redis       is also 54.219.88.28 outside /10.222.103.158 inside
Graphite    is 54.215.1.135 outside      /10.160.150.32 inside
Mysql       is 54.215.110.164 outside    /10.174.121.164 inside
Key service is 184.169.153.214 outside   /184.169.153.214 inside
Redis       is , no outside              /10.222.103.158 inside
API         is 54.215.88.47 outside      /10.198.43.98 inside
S3          is  


Cassandra "seeds" are: 	10.222.101.109 
			10.222.139.147 
			10.174.77.69

Cassandra other IPs are: 10.174.49.58
    disable all - should fail
    disable any one or two - should work


HDFS uses all of these IPs:
HDFS primary namenode is 10.196.84.183 inside, no outside
54.241.90.37 outside / 10.198.10.241
50.18.12.53 outside / 10.198.10.249
50.18.234.20 outside / 10.197.17.172
54.215.60.139 outside /10.196.89.152

The DR does not use Hive or MySQL. The API uses MySQL when helping the
DR, so you should include cutting off the API's access to MySQL.

==============================================================================================================




show partitions summary_persist_mapper;



---Error Summary---
Note that you will see that there are a log of bogus errors. That's fine. The point of this is to put them in our faces so that we will actually fix them. Errors that aren't really errors are just like the boy who cried wolf. They are noise that prevent us from identifying real issues. Possible Query:
>select error_message, min(time) as first_occurence, max(time) as last_occurence, count(*) as count from summary_careopt_errors where day=2 and month=4 group by error_message order by count desc

--Load Summary--
Unfortunately, we have no org or user information here. So, all we can really do is a simple summary. For this we will summarize total number of patient loads, load times, patient sizes, average throughput and cache size:
select count(*) num_loads,  avg(cassandra_load_millis) / 1000 as avg_load_time_seconds, max(cassandra_load_millis) / 1000 as max_load_time_seconds, avg(patient_bytes) / 1048576 as avg_patient_mb, max(patient_bytes) / 1048576 as max_patient_mb, avg((patient_bytes / cassandra_load_millis) * 1000) / 1048576 avg_mb_per_second, min(patient_cache_size) as min_patient_cache, max(patient_cache_size) as max_patient_cache from summary_careopt_load

--Search Summary--
This only occurs when a search is actually performed and can be costly due to the expense of the deferred Lucene indexing. We don't have a proper org id here but it is encoded in the username (id_orgId). So, we will split this and group by org. This doesn't work perfectly because many email addresses (including our internal "hcc" and "indexer" addresses, as well as MMG) actually have an underscore in them, and users logging in directly won't have one at all, so you will have to work out a solution to that. We also don't have bytes so its hard to understand the values of the patient search times relative to size but this is a start:
>select split(username, "_")[1] as org, count(distinct split(username, "_")[0]) as end_users, count(distinct patient_sql_id) as num_patients, min(patient_access_millis) as min_time, max(patient_access_millis) as max_time, avg(patient_access_millis) as avg_time, min(time) as first_access, max(time) as last_access from summary_careopt_search  where day=2 and month=4 group by split(username, "_")[1] order by num_patients desc


=========================================================================================================================================
To make 503 happen, put the DR into stopped state. This means that no
uploads can happen. The service start/stop scripts use this feature.

Here's how it works:
to put it in 'active' state, do a POST with this:
https://...../receiver/control/active
and a token part. It's just like a normal upload but without document
and catalog, just the token part. It will return JSON like this:
{"time":1396042658935,"datestamp":"2014-03-28T21:37:38.935Z","state":"active",
"processId":2993,"numSessions":0}

This means that it accepts doc uploads and there are no current
uploads (numSessions = 0)

To put it in 'stopped' state, do a POST with this:
https://...../receiver/control/stopped

It will return a Json that has 'active' or 'stopped'. 'stopped' does
not wait for it to stop. You need to pause maybe 5 seconds and post
'stopped' again.

You can get a 404 by using a bogus URL. 404 means 'not found'.

=========================================================================================================================================

/usr/lib/apx-reporting/assets/reports/production/pipeline/2014/3/10.html

for example

/usr/lib/apx-reporting/assets/reports.txt

/usr/lib/apx-reporting/assets

https://reports.apixio.com/html/

https://reports.apixio.com/html/reports.html

https://reports.apixio.com/html




=================== Coordinator re-submitting failed jobs =================

Coordinator Logs:
tail -f -n 200 /var/log/apx-coordinator/apx-coordinator.log

Coordinator machine - indexer machine on staging ...

cd /usr/lib/apx-coordinator/bin

. setcp

hadoop fs -ls /user/apxqueue/queue-location-14/failed-jobs/

=============================================== resubmit =======================================================================================

java com.apixio.coordinator.cmdline.ResubmitJob --list

usage:
functions
 --list	         Display information about selected failed jobs.
 --submit	 Resubmit selected jobs as new work. 
 --ignore	 Forget the jobs indicated were failed. 
                 *Currently irreversible; falsely ignored jobs have to be submitted manually.  

arguments
 --all	                         Select all jobs with a status of FAILED.
 --job <jobId>	                 Select jobs with the given space separated ID(s).
 --type <failedJobType>	         Select jobs with the given type: 'failed', 'orphaned', or 'launcherror'.
 --activity <activityType>	 Select jobs of the given space separated activity type(s): 'parser', 'ocr', 'persist', etc ...
 --org <orgId>	                 Select jobs from the given organization. STAGING ONLY



=============================================== activity config ==============================================================================

java com.apixio.coordinator.cmdline.ActivityConfigTool --list

java com.apixio.coordinator.cmdline.ActivityConfigTool --set-priority "trace=8 ocr=7 persist=6 parser=5"

    --delete <arg>           Delete config for one/some/all activity
                             name(s)

    --disable <arg>          Disable activity (don't schedule) for
                             one/some activity name(s)

    --enable <arg>           Enable activity (do schedule) for one/some
                             activity name(s)

    --help                   Print help information and exit

    --list <arg>             List config for one/some/all activity name(s)

    --set-priority <arg>     Set activity priority: 
                             name=priority ...

    --set-slot-alloc <arg>   Set non-job-specific slot allocation:
                             name=allocated,borrowable ...

    --set-slot-limit <arg>   Set job-specific slot limits:
                             name=min,max ...

=============================================== job Info <Staging> ===========================================================================

java com.apixio.coordinator.cmdline.JobsInfo --queued --workflow /etc/apx-coordinator/pipeline.xml


 	--running       show jobs that are running on cluster

        --queued [csv]  show jobs queued for slot assignment; optional:  limit to given types

        --launching     show jobs waiting to be submitted to cluster

        --stats         show runtime slot allocation statistics

        --workflow      file required if --queued doesn't specify types


==============================================================================================================================================
etc.

Some invalid command line usage won't be detected before spring starts up so you could see the init lines before seeing an invalid usage message.

The --type orphaned won't pick up any jobs that are orphaned prior to this code release.

- Activity Config 
- Jobs Info <STAGING> 
- Org Config 
- Ps 
- Resubmit Job 
- Show Launchable 
- Submit Work Command 
- System Config <STAGING> 
- Walk Tree


============================== LANCE ==========================================

503 service unavailable
400 bad request
500 internal server error
401 authentication error

============================================================




a70f3727-3186-48ac-99cf-ab0c729de205	sourcesystem	apxdemop0005	8b7e9786-9ee9-4527-9139-34a88a414d47		10000275	10000275_backload	JPG	2013-11-15T23:11:51Z
56b3f352-5058-461c-b544-ab5a96480580	sourcesystem	apxdemop0005	d94afb78-7b1b-4b3e-a994-4b0bc810d7ea		10000275	10000275_backload	DOC	2013-11-15T23:11:53Z
72d7f6c1-efec-47f5-bd28-9836ff1f8efa	sourcesystem	apxdemop0005	3516510c-525a-44bb-bad9-214dac4c3c06		10000275	10000275_backload	RTF	2013-11-15T23:11:55Z
d4e434fc-3fe8-47be-ab9d-8a6e3eb926f7	sourcesystem	apxdemop0005	5f5b976c-acd2-4a47-9807-23c83b27a327		10000275	10000275_backload	TXT	2013-11-15T23:11:56Z


DATA=test_manifest.csv
curl -3 -s -k -X PUT --form token=$TOKEN --form file=@$DATA  "$HOST/receiver/batch/$BATCH/manifest/$DATA/upload"

===================================== JENKINS on STAGING ==================================================================

https://stagegw.apixio.com:8075/

You are ishekhtman with normal apixio 

pw: apixio.123

you should be able to just change directory to /mnt and work just like you did before.
[2:03:04 PM] Lance N (Apixio): ssh -i ~/secrets/id_rsa.pem ishekhtman@ip-10-170-181-208 date
[2:05:18 PM] Lance N (Apixio): ssh -i ~/secrets/id_rsa.pem ishekhtman@ip-10-170-181-208 "cd /mnt/lance/code; sh token.sh"

tail -f /usr/share/apache-tomcat-7.0.47/logs/catalina.out

ps -aux
ps -aux | grep apixio
kill -9 .......

ps -ef | grep java
tomcat `number`
kill -9 `number`
sudo service tomcat start

503 service unavailable
400 bad request
500 internal server error
401 authentication error

allergies
problems
procedures
medication
immunizations
labs
officenotes


===========================================================================================================================



I don't actually need uncompressed, unencrypted size. Since Cassandra is what went down, I'm actually only concerned with what is actually written to Cassandra, which appears to be in the '$.file.bytes' JSON of the persist EVENT.

Also, I am looking for the size of the LARGE objects (primarily the 300MB document that originally failed), not MMG.


ORGID in Production

STAGING
H1/NN = 54.215.109.178 / 10.196.84.183
H2/JT = 54.215.31.198 / 10.198.4.49
H3/HIVE = 184.169.209.24 / 10.196.47.205
H4 = 54.241.90.37 / 10.198.10.241
H5 = 50.18.12.53 / 10.198.10.249
H6 =50.18.234.20 / 10.197.17.172
H7 =54.215.60.139 / 10.196.89.152


PRODUCTION
10.197.61.199 / 54.241.193.32
10.198.21.135 / 54.215.57.170



https://localprodapi.apixio.com/API/v1/auth/token/?username=apxdemot0138&password=Hadoop.4522

https://prodapiext.apixio.com/API/v1/auth/token/?username=apxdemot0138&password=Hadoop.4522

update host file to view Hadoop Logs: c:\windows\system32\drivers\etc\hosts



=============================== HADOOP JOB-TRACKER =====================================================================================

PRODUCTION:

http://54.219.56.133:50030/jobtracker.jsp


STAGING:

54.215.31.198:50030/jobtracker.jsp
http://54.215.31.198:50030/jobtracker.jsp


=============================== CHECKING DISK SPAVE ON LINUX MACHINE ===================================================================

df -h

================================= OFFICE QA CLOUD MACHINE ===============================================================================

QA test machine

ec2-54-219-117-239.us-west-1.compute.amazonaws.com



you can log in through office/vp
I can add your home IP addy if its not 

184.169.129.122 or
76.220.21.93

C:\Users\Igor\Documents\Apixio\HADOOP\INDEXER_SERVER_MACHINE\key\new_staging.ppk

cp -avr /mnt/testdata/SanityTwoPatientsAllFileTypesNoOCR/Catalogs/20131025_105656 /mnt/indexer$i/SOURCE/

=============================== PRODUCTION DR ===========================================================================================

uploadurl=https://dr.apixio.com:8443/


============================== CHNGING ORG SPECIFIC PRIORITIES ===========================================================================

setting coordinator slot configuration

Coordinator slot config:
java com.apixio.coordinator.cmdline.SlotConfigTool --set “activity=1"
(check the config)
java com.apixio.coordinator.cmdline.SlotConfigTool --listall
[1:31:45 PM] Lilith Schneider: from bin after . setcp






connect to coordinator server (indexer server):
root@54.219.88.28 
cd 
cd /usr/lib/apx-coordinator/bin

. setcp

java com.apixio.coordinator.cmdline.OrgConfigTool --set "57=6"

java com.apixio.coordinator.cmdline.OrgConfigTool --listall

=============================== RESTARTING COORDINATOR ===================================================================================

service apx-coordinator start
service apx-coordinator stop
service apx-coordinator status

or restart

or status (to see if started or not)

============================= REPLACING STRINGS IN FILES =================================================================================

forfiles /P "###FOLDER_WITH_FILES_TO_REPLACE###" /M *.apx /S /C "cmd /C ###DIRECTORY_WHERE_VBS_FILE_IS###\replace.vbs @path "###STRING_TO_FIND###" "###STRING_TO_REPLACE_WITH" /I"


forfiles /P "C:\Apixio\Indexer\WORK\error" /M *.apx /S /C "cmd /C C:\Apixio\Indexer\SOURCE\replace.vbs @path "\\nextgendimirror\d$\NextGen" "\\nextgendi2\NextGenRoot" /I"


forfiles /P "Z:\TestData\99808Documents5OrgsStressTest\Catalogs\temp\201310140447072632" /M *.apx /S /C "cmd /C C:\Users\Igor\Documents\Apixio\Alex\replace.vbs @path "/root/test/source/org1/docs/201310140447056091/" "\\nas.apixio.com\Public\TestData\99808Documents5OrgsStressTest\Documents\201310140447056091\" /I" 



C:\Users\Igor\Documents\Apixio\Alex


forfiles /P "Z:\TestData\99808Documents5OrgsStressTest\Catalogs\temp\201310140447072632" /M *.apx /S /C "cmd /C C:\Users\Igor\Documents\Apixio\Alex\replace.vbs @path "/root/test/source/org1/docs/201310151021370080/" "\\nas.apixio.com\Public\TestData\99808Documents5OrgsStressTest\Documents\201310151021370080\"


cp -avr //nas.apixio.com/Public/TestData/SanityTwoPatientsAllFileTypes/Catalogs/ /root/mnt/test/indexer1/source


xcopy Z:\TestData\SanityTwoPatientsAllFileTypes\Catalogs\*.* Z:\Indexer\SOURCE\*.* /s/e/c



============================== CONFIGURING COORDINATOR =====================================================================================

- The tool is com.apixio.coordinator.cmdline.SlotConfigTool

- If you run it without any input or --help, it will give you the usage

- in your case, you should use --set ocr=10    (anything higher than the number of sequence files will do until we fix this design issue)

- if you want to see if your setup work, you can use --listall


============================== CONFIGURING LOCAL DNS ON WINDOWS MACHINE =====================================================================

With Administrative priviledges enabled,

edit hosts file located in C:\Windows\System32\drivers\etc\hosts

append a new line to it for example:

10.19.220.86     nas

10.19.220.86     BlackPants

10.19.220.86     nas.apixio.com

============================= PROPER WAY TO KILL COORDINATOR ===================================================================================

ps -ef | grep java

ps -ef | grep Coordinator

kill -9 (process# ex. 28485)

pwdx (process#) - will tell you which direcory coordinator was started

================================= CLEAR REDIS QUEUE ============================================================================================

redis machine: 54.219.54.64

root pem will work for this

or root ppk

then we shud do /opt/redis/redis-cli <<< flushdb

=========================== COPYING FILES FROM OLD STAGE CLUSTER HDFS TO THE NEW ===============================================================



hadoop jar apixio-cassandra-utils-1.0.7-SNAPSHOT.jar migrate -files migration.properties,apixio-security.properties

184.169.139.118,184.169.171.87,184.169.142.180,184.169.168.150 cf10000652 /user/apxqueue/igor_migration_652


igor-migration/migration


hadoop distcp hdfs://10.176.235.204:8020/user/apxqueue/igor_migration_534 hdfs://10.196.84.183:8020/user/apxqueue/igor_migration_534




http://ec2-54-215-31-198.us-west-1.compute.amazonaws.com:50030/jobtracker.jsp

hadoop distcp hdfs://10.20.100.114:8020/user/apxqueue/saved hdfs://10.196.84.183:8020/user/apxqueue/

hadoop distcp hdfs://10.176.235.204:8020/user/apxqueue/saved hdfs://10.196.84.183:8020/user/apxqueue/

hadoop distcp hdfs://10.176.235.204:8020/user/apxqueue/saved/APXDEMOT87187250MB211SEQFLS hdfs://10.196.84.183:8020/user/apxqueue/saved/APXDEMOT87187250MB211SEQFLS


============================================================== SQUIRREL + HIVE ===============================================================

Parser Table - staging_logs_doc2apo_epoch

Ocr Table - staging_logs_ocr2apo_epoch

Persist Table - staging_logs_apo2persist_epoch

OLD PIPELINE LOG TABLES:
apo2persist
doc2apo
ocr2apo






select * from staging_logs_doc2apo_epoch where get_json_object(line, '$.jobname') = 'DocImport-IGOR-2MMG-TEST1-parser'

jobname":"DocImport-IGOR-2MMG-TEST1-parser

select * from staging_logs_doc2apo_epoch where get_json_object(line, '$.jobname') = 'DocImport-IGOR-2MMG-TEST1-parser' and get_json_object(line, '$.file.name') = '3235313831323031353139323635353830313937343456303163374532346E375A4E416A2B2F6972324D43757A2F79672B30796B39517343435A6C5A4E5A50776C527479386C376532733649564C354765696C6E734E49737A68'



===============================================================================================================================================

select count(*) from igor_staging_ocr_analysis;
select * from igor_staging_ocr_analysis;

drop table igor_staging_ocr_analysis;

CREATE EXTERNAL TABLE igor_staging_ocr_analysis(line STRING) ROW FORMAT
DELIMITED LINES TERMINATED BY '\n' 
STORED AS TEXTFILE
LOCATION '/user/apxqueue/staging_logs/ocr2apo/2013-07-23';

select count(*) from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'success';
select count(*) from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'error';
select count(*) from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'skipped';

select get_json_object(line, '$.document.ocr.status') from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'success';
select get_json_object(line, '$.document.ocr.status') from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'error';
select get_json_object(line, '$.document.ocr.status') from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'skipped';

select get_json_object(line, '$.document.patientuuid') from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'success';

select get_json_object(line, '$.document.patientuuid'), get_json_object(line, '$.document.orgname'), get_json_object(line, '$.document.ocr.status') from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'success';
select get_json_object(line, '$.document.patientuuid'), get_json_object(line, '$.document.orgname'), get_json_object(line, '$.document.ocr.status') from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'error';
select get_json_object(line, '$.document.patientuuid'), get_json_object(line, '$.document.orgname'), get_json_object(line, '$.document.ocr.status') from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'skipped';
select get_json_object(line, '$.document.patientuuid'), get_json_object(line, '$.document.orgname'), get_json_object(line, '$.document.ocr.status') from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'failed';
igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'skipped';

select * from staging_logs_ocr2apo_epoch where get_json_object(line, '$.datestamp') like '2013-08-14%' and get_json_object(line, '$.jobname') = 'DocImport-IGOR-OCR3-LOGGING-ocr' and 
get_json_object(line, '$.documentuuid') = 'f7903594-45e6-4aa6-afb9-827b146b1f12';

1. cass.sh, 2. use apixio; , 3. get columnfamily['pat:uuid'];

================================================================================================================================================



create table

CREATE EXTERNAL TABLE staging_pipeline_analysis(line STRING) ROW FORMAT
DELIMITED LINES TERMINATED BY '\n' 
STORED AS TEXTFILE
LOCATION '/user/apxqueue/staging_logs/pipeline';

create external table igor_pipeline_test (line string) location '/user/apxqueue/staging_logs/pipeline';

CREATE EXTERNAL TABLE igor_staging_pipeline_analysis(line STRING) ROW FORMAT
DELIMITED LINES TERMINATED BY '\n' 
STORED AS TEXTFILE
LOCATION '/user/apxqueue/staging_logs/pipeline';
select * from igor_staging_pipeline_analysis limit 5;


07-10-2013 ------------------------------------------------------------------

CREATE EXTERNAL TABLE igor_staging_ocr_analysis(line STRING) ROW FORMAT
DELIMITED LINES TERMINATED BY '\n' 
STORED AS TEXTFILE
LOCATION '/user/apxqueue/staging_logs/ocr2apo';

select * from igor_staging_ocr_analysis;

select * from igor_staging_ocr_analysis where get_json_object(line, '$.apxdemot870.ocr.status') like 'success';

drop table igor_staging_ocr_analysis;

hdfs -rmr staging_logs/ocr2apo/*



select * from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'success'; - how many documents are there total in success
select * from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'error';
select * from igor_staging_ocr_analysis where get_json_object(line, '$.document.ocr.status') like 'skipped';


======================================================== GRAPHITE and FLUENT AND OCR ==========================================================

## Graphite configuration

GRAPHITE_URL=10.160.150.32
GRAPHITE_PORT=2003
GRAPHITE_METRICS_PREFIX=staging.
GRAPHITE=true

JAVAOPTS=-Xmx1536m -XX:ErrorFile=/tmp/java_error%p.log -XX:MaxPermSize=256m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -verbose:gc -Xloggc:/tmp/@taskid@.gc -DFLUENT_HOST=10.222.135.151 -DFLUENT_TAG=staging

hadoop fs -ls /user/apxqueue/staging_logs
this shows the current fluentd saved.
the latest one named 2013......_18 is the current hour-long block (in GMT time)
This will have one file with a long name. When the pipeline runs this file should get longer and longer



http://graphite.apixio.com/dashboard/#Staging Pipeline
this shows graphite data sent


========== TO OCR : ===============================================================

igor2-apixio-pipeline/sequence-utils/bin
 
./sf ocr pipeline-igor22/general/p1/output/*/ocr pipeline-igor22/general/p1/ocrd_output

./sf ocr pipeline-igor22/general/p1/output/2013613_*/ocr pipeline-igor22/general/p1/ocrd_output

/user/apxqueue/pipeline-igor22/general/p1/output/2013614_ApixioPipeline-IGOR-OCR-2ORG-TEST-1_1371232919158

./sf ocr pipeline-igor22/general/p1/output/2013614_*/ocr pipeline-igor22/general/p1/ocrd_output

- Will not run from screen-------------



Then take what's in the output folder "pipeline-net/ocr_from_igor" and move it to /general/queue/ and process upload through pipeline


hadoop fs -cp /user/apxqueue/pipeline-igor22/general/p1/ocrd_output/* /user/apxqueue/pipeline-igor22/general/p1/queue/


============================================ STEPS for APO UPLOAD TESTING ============================================================

1.) 

Run Pipeline:
POLL_OCR_UPLOADQUEUE=false
DO_QUEUE_MGMT=true
SCHEDULE_HADOOP_JOB=true


2.) 

Run Pipeline: 
POLL_OCR_UPLOADQUEUE=true
DO_QUEUE_MGMT=true
SCHEDULE_HADOOP_JOB=false

verify that the sequence file exists in:
hadoop fs -ls /user/apxqueue/pipeline-igor16/ocr/p1/queue/

3.)

change run.sh to:
hadoop jar apixio-sfilecreator-0.0.2-SNAPSHOT.jar ocr -files libtessbase.so,apixio-security.properties -archives tessdata.tar.gz#tessdata -Dmapred.map.child.java.opts="-Xmx1536m" /user/apxqueue/pipeline-igor16/ocr/p1/queue/ /user/apxqueue/pipeline-igor16/ocr/p1/output/
 
Run IGOR-OCR, input is OCR/P1
./run.sh

move files from OUPUT to GENERAL queue:
hadoop fs -mv /user/apxqueue/pipeline-igor16/ocr/p1/output/* /user/apxqueue/pipeline-igor16/general/p1/queue/



4.) 

Delete all files from OCR from OCR folder so that pipeline does not pick it up
hadoop fs -rmr /user/apxqueue/pipeline-igor16/ocr/p1/queue/*

Run Pipeline with all true
POLL_OCR_UPLOADQUEUE=true
DO_QUEUE_MGMT=true
SCHEDULE_HADOOP_JOB=true

========================================================== LOG ME IN ===============================================================

www.logmein.com
user: ops@apixio.com
pw: 8p1log19..

igor
apixio12

============================================================ MONITOR SETUP =========================================================

/igor-monitor (on stage-cluster)

./monitor.sh (no need to run. will be executed via cron job)

crontab -e (to edit settings)
press <i> to be in editable mode
go to end of file
change csv filename
<esc>, <:>, <w>, <q>, <enter> to exit and save.
<esc>, <:>, <q>, <enter> to quit without saving.

test.apixio.com/stagedashboard/igortestlogs1.csv - to change view

chmod 777 monitor.sh
nano monitor.sh

delete from stage_monitoring


- Create folder in Google Docs and save all statistics there with date label added ("Staging Pipeline Statistics")
- Change settings of 1st column in spreadsheet to point to the correct CSV file (every time)

hadoop fs -copyFromLocal /home/apxqueue/igor-testing/Pipeline_Troughput_Testing/TEST1_500MB_SEQ_FILES/* /user/apxqueue/pipeline-igor1/general/p1/queue
hadoop fs -copyFromLocal /home/apxqueue/igor-testing/Pipeline_Troughput_Testing/TEST2_64MB_SEQ_FILES/* /user/apxqueue/pipeline-igor1/general/p1/queue
 
 
GENERAL_NAME=general
GENERAL_URI=/user/apxqueue/pipeline-igor1/general/p1/queue
GENERAL_URI_SUCCESS=/user/apxqueue/pipeline-igor1/general/p1/success
GENERAL_URI_ERROR=/user/apxqueue/pipeline-igor1/general/p1/error
GENERAL_URI_PROCESSING=/user/apxqueue/pipeline-igor1/general/p1/processing
OCR_NAME=default
OCR_URI=/user/apxqueue/pipeline-igor1/ocr/p1/queue
OCR_URI_SUCCESS=/user/apxqueue/pipeline-igor1/ocr/p1/success
OCR_URI_ERROR=/user/apxqueue/pipeline-igor1/ocr/p1/error
OCR_URI_PROCESSING=/user/apxqueue/pipeline-igor1/ocr/p1/processing
OCR_SOURCE=/user/apxqueue/ocr-queue

/user/apxqueue/uploads/apxdemot751/sourcesystem/ocr/P1


hit@test.apixio.com:/var/lib/tomcat6/webapps/stagedashboard/igortestlogs2.csv (location of the csv file on Test Server)


After running both jobs, copy /home/apxqueue/monitor/monitor.sh to /home/apxqueue/igor-monitor/

==================================================================================================================================

184.169.168.150 - cassandra stage-cluster
mysqlHost=10.176.255.6


cassandraHost=184.169.139.118:9160,184.169.171.87:9160,184.169.142.180:9160,184.169.168.150:9160

demo.apixio.com - demo server

chmod 777 run.sh

DEMO-CASSANDRA
10.188.157.166 - int (use)
50.18.252.12 - ext
"This organization is an HCC Optimizer ONLY client (HCC Mode). New user account activation will automatically grant HCC Optimizer access privileges."

============================ Pipeline Update from Jenkins ===============================

apixio-pipeline-trigger-package
this is the latest one
this will transfer the packaged pipeline to staging cluster in the jenkins_v3 directory

======================== Indexer Update =================================================

- Copy Latest Indexer (zipped) Jar from Jenkins (2nd file in the list of 2) to V20 foler
- Remove SNAPSHOT from the copied file name

=========================================================================================
=========================================================================================
=========================================================================================
=========================================================================================
=========================================================================================
======================== Indexer Account and PW Update ==================================
=========================================================================================
=========================================================================================
=========================================================================================
=========================================================================================
=========================================================================================


CARE OPTIMIZER (STAGING):
https://supweb.apixio.com/signup.do?action=loadSignup
uploadurl=https://supload.apixio.com:8443/




CARE OPTIMIZER (PRODUCTION):
https://care.apixio.com/signup.do?action=loadSignup
uploadurl=https://dr.apixio.com:8443/




# TEST:
# https://test.apixio.com/signup.do?action=loadSignup

# DEMO:
# https://demo.apixio.com/signup.do?action=loadSignup


CD C:\_Pipline\Indexer\V30

CD C:\_Pipeline\Indexer\V30


java -cp apixio-indexer-3.0.0-SNAPSHOT.jar com.apixio.indexer.util.Setup

java -cp apixio-indexer-3.1.0.jar com.apixio.indexer.util.Setup

java -jar apixio-indexer-3.1.0.jar


User: apxdemot463

pw: H....4x2x (you know what it is)

key: V01_8p1x1o1825sanmateocalifornia9440

production key: V01_dWbZtN56Hnwez6jR5Tfjx25bdfj8GVJt


PRODUCTION:

http://54.219.56.133:50030/jobtracker.jsp


STAGING:

54.215.31.198:50030/jobtracker.jsp
http://54.215.31.198:50030/jobtracker.jsp



========================= NEW PRODUCTION =================================================

Enter UserName:
newprodtest@apixio.net
Enter PassWord:
Hadoop.4522
Enter Key:
V01_8p1x1o1825sanmateocalifornia9440

# orgID = 10000232 (Anthony - no good)

orgID = 10000268

http://54.219.56.133:50030/jobtracker.jsp
(production jobtracker link)
========================= Pipeline Update from Jenkins ===================================
 
cd igor1-apixio-pipeline/
cd pipeline-trigger/

============= EITHER

cp /home/apxqueue/jenkins/pipeline-trigger/apixio-pipeline-trigger-2.1.1-SNAPSHOT.jar .

============= OR

cp /home/apxqueue/jenkins_v2/apixio-pipeline/pipeline-trigger/apixio-pipeline-trigger-2.1.2-SNAPSHOT.jar .


hadoop fs -cp commsys_backup/mmg_queue/queue/* /user/apxqueue/pipeline-igor/queue/

C:\Users\Igor\Documents\Apixio\PHI\SUTTER\Anthony\docs

==================== saved 4 sequence files 64Mb 13,106 MMG CCR apxdemot292 ===========

/user/apxqueue/uploads/SAVEDSEQ/13106MMGCCR 


hadoop fs -cp /user/apxqueue/pipeline-igor/queue/* /user/apxqueue/uploads/SAVEDSEQ/13106MMGCCR


hadoop fs -cp /user/apxqueue/uploads/SAVEDSEQ/13106MMGCCR/* /user/apxqueue/pipeline-igor/queue


==================== saved 2 sequence files 10 MMG with Documents and Images ===========

/user/apxqueue/uploads/SAVEDSEQ/10MMGWDOCS

hadoop fs -cp /user/apxqueue/pipeline-igor/queue/* /user/apxqueue/uploads/SAVEDSEQ/10MMGWDOCS

hadoop fs -cp /user/apxqueue/uploads/SAVEDSEQ/10MMGWDOCS/* /user/apxqueue/pipeline-igor/queue

================================================================================================


421 MMG Files

hadoop fs -cp /user/apxqueue/commsys_backup/mmg_queue/queue/* /user/apxqueue/pipeline-igor/queue/

/user/apxqueue/pipeline-igor/error


/user/apxqueue/igor

# saving 30 error files to /igor for future testing of OCR

hadoop fs -copyToLocal /user/apxqueue/pipeline-igor/error/* /home/apxqueue/igor/421_MMG/error/

hadoop fs -copyToLocal /user/apxqueue/pipeline-igor/logs/* /home/apxqueue/igor/421_MMG/logs/
================================================================================================

hadoop fs -copyToLocal /user/apxqueue/pipeline-igor1/queue/* /home/apxqueue/nethra/wrong_size_seq_files/

/user/apxqueue/uploads/MMG/NextGen-1k/P1

================================================================================================

hadoop fs -copyToLocal /user/apxqueue/pipeline-igor1/error/* /home/apxqueue/igor/1000_Anthony_generator_job/error/


======================= 20 MMG (COMPLETE) PATIENTS UPLOAD FROM ALEX ===========================

/user/apxqueue/uploads/MMG/NextGen-20/P1 - source folder




===================== SAVED 5 ZIPPED FILES FROM 1000 Anthony Generator JOB ====================

hadoop fs -copyFromLocal /home/apxqueue/igor/1000_Anthony_generator_job/error/* /user/apxqueue/uploads/igor1testorg/igor1sourcesystem/P1

apxdemot316 -> apxdemot341


/home/apxqueue/igor/1000_Anthony_generator_job/error



===================== SAVED 30 ZIPPED FILES FROM 421 MMG JOB ==================================


hadoop fs -copyFromLocal /home/apxqueue/igor/421_MMG/error/* /user/apxqueue/uploads/igor1testorg/igor1sourcesystem/P1

apxdemot -> apxdemot342

/home/apxqueue/igor/421_MMG/error


============================ CREATING NEW CASSANDRA COLUMN FAMILY ==============================

Login through Putty:
184.169.169.1 (STAGING-CASSANDRA)
key(saved in hadoop/cassandra)
user: root

NEW STAGE CLUSTER:
54.219.24.211
cqlsh




cd apache-cassandra-1.1.1

cd bin

./cassandra-cli

use apixio;

################ replace with correct column_family name ######################

create column family cf10000842
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and read_repair_chance = 0.1
  and gc_grace = 864000
  and replicate_on_write = true
  and caching = 'KEYS_ONLY'
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor'};




############## RESPOND BACK RESULTS SHOULD BE: ############################################
cdb60777-ed97-305a-b90d-c558c9a7e163
Waiting for schema agreement...
... schemas agree across the cluster


--------------------------------------------------------------------------------------------

cassandra.output.columnfamily = igor_migration_test691
cassandra.trace.columnfamily = igor_migrationtrace_test691

cqlsh

use apixio;

CREATE TABLE igor_migrationtrace_test691 ( 
         rowkey text, 
         col text,
         d blob,
         PRIMARY KEY (rowkey, col)
         ) WITH
         comment = 'table with dynamic columns' AND
         caching = 'KEYS_ONLY' AND
         bloom_filter_fp_chance = 0.1 AND
         read_repair_chance = 0.1 AND
         dclocal_read_repair_chance = 0.0 AND
         gc_grace_seconds = 864000 AND
         replicate_on_write = 'true' AND
         compaction = {
                          'class' :  'LeveledCompactionStrategy',
                          'sstable_size_in_mb' : 256,  'tombstone_compaction_interval' : 1, 'tombstone_threshold' : 0.2 
                        } AND
         compression = {'sstable_compression' : ''};
		
		
		create keyspace apixio
  with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
   and strategy_options = {replication_factor:1}
   and durable_writes = true;





==================================== DataStax Access VERIFICATION =============================================


http://localhost:8888/opscenter/index.html

NEXUS.APIXIO.COM


=============================== ADDING RECORD IN MYSQL FOR NEW COLUMN FAMILY ==================================


- Using MySQL Workbench log into: 184.169.234.154 (wrong, old)

- Using MySQL Workbench log into: 50.18.142.206
(u: root, pw: ap.api19)


- find user in [user] table get UID
- find ldap_org_id in [user_org] table
- add a record in [org_properties]
- restart TOMCAT on TEST machine


#############################################################################################
All four files at this point should be inflated and placed into /home/hit/igor/temp/ folder:
  inflating: /home/hit/igor/temp/account.txt
  inflating: /home/hit/igor/temp/indexerMetrics.txt
  inflating: /home/hit/igor/temp/manifest.txt
  inflating: /home/hit/igor/temp/uploaderMetrics.txt
#############################################################################################


1.) Copy manifest.txt (Indexer Manifest) from DOC-RECEIVER SERVER to local machine:
Using WinSCP, log into DOC-RECEIVER server and copy manifest.txt file to local folder (c:\users\igor\documents\apixio\hadoop\cassandra\manifest)
- Drag-n-Drop

2.) Copy manifest.txt (Indexer Manifest) from local machine to (HADOOP STAGING-CLUSTER):
using WinSCP, log into STAGING-CLUSTER server and copy manifest.txt file from local machine to (/home/apxqueue/igor/)
- Drag-n-Drop

cat /home/hit/igor/temp/manifest.txt (to view file content)


---------------------- NEW STEPS, COPYING ZIPPED FILES FROM HDFS TO LOCAL --------------------------------------------

hadoop fs -copyToLocal /user/apxqueue/indexer_logs/apxdemot644/sourcesystem/1361415432579.zip /home/apxqueue/igor/

hadoop fs -copyToLocal /user/apxqueue/indexer_logs/apxdemot644/sourcesystem/* /home/apxqueue/igor/

hadoop fs -copyToLocal /user/apxqueue/pipeline-igor2/colfam456/compare_output/part-r-00000 /home/apxqueue/igor/

hadoop fs -copyToLocal /user/apxqueue/igor/patient_doc_ids /home/apxqueue/igor/


====================================== CASSANDRA MANIFEST ============================================================


1.) Log into Stage-Cluster using NoMachine
2.) From top Menu select "Places" then select/open [Home] folder
3.) Within [Home] folder select [Igor} folder and within [Igor] folder select/open [patientmanifest] folder


4.) select patientmanifest.pig, right click (open with "Text Editor") and make changes in two (2) places and save file:
- Change "%default cfname 'igor_qctool8';" to my column_family "igor_qctool8"
- Change "store manifest_rows_result into '/user/apxqueue/pipeline-igor1/igor_qctool8' using PigStorage('\t');" (to store in my pipeline, my column-family "igor_qctool8")

Open terminal: cd igor/patientmanifest/

Note: run.sh has to have correct cassandra address (usually no changes need to be made)


5.) (open new terminal and type, copy/paste will not work):  cd igor/patientmanifest/
6.) ./run.sh (this will create Cassandra Manifest and will store it in /user/apxqueue/pipeline-igor1/igor_qctool10)
-rw-------   3 apxqueue apixio      30644 2012-12-18 19:30 /user/apxqueue/pipeline-igor1/igor_qctool10/part-m-00000
-rw-------   3 apxqueue apixio      10400 2012-12-18 19:30 /user/apxqueue/pipeline-igor1/igor_qctool10/part-m-00001


7.) PuTTy open (STAGE-CLUSTER) and combine output from all files into one file names (cassandra_manifest) on local folder
hadoop fs -cat /user/apxqueue/pipeline-igor3/cf10000484/* > /home/apxqueue/igor/cassandra_manifest
########################################################################################################
At this point both manifest files should be located in /home/apxqueue/igor/ folder on STAGE-CLUSTER
########################################################################################################


8.) From top Menu select "Places" then select/open [Home], [igor], [patientmanifest] folder (STAGE-CLUSTER)
9.) Edit validation_report.pig (in three (3) places) and SAVE:
- indexer_output = LOAD '/user/apxqueue/pipeline-igor1/igor_qctool7/menifest.txt' AS (patientUUID:bytearray, sourceDocumentName: chararray,psize:
- cass_output = LOAD '/user/apxqueue/pipeline-igor1/igor_qctool7/cassandra_manifest' AS (patientUUID:bytearray, sourceDocumentName: 
- store missing_doc_names into '/user/apxqueue/pipeline-igor1/igor_qctool7/compare_output'; 

10.) Copy both manifest files from local to HDFS (Stage-Cluster):
- Open Putty (STAGE-CLUSTER) and run the following 2 commands (copy both indexer and cassandra manifest files int HDFS):
a.) hadoop fs -copyFromLocal /home/apxqueue/igor/manifest.txt /user/apxqueue/pipeline-igor3/cf10000484/
b.) hadoop fs -copyFromLocal /home/apxqueue/igor/cassandra_manifest /user/apxqueue/pipeline-igor3/cf10000484/


11.) Run manifest files comparison (PIG SCRIPT):
- open new terminal on STAGE-CLUSTER (NoMachine)
- cd igor/patientmanifest
- ./run_validation.sh

12.) View results of the QCTOOL (differencs between the two manifest files):
- Open Putty - STAGE-CLUSTER and run the following 2 commands (replace with appropriate folder name):
a.) hadoop fs -ls /user/apxqueue/pipeline-igor2/colfam456/compare_output
b.) hadoop fs -cat /user/apxqueue/pipeline-igor2/colfam456/compare_output/part-r-00000


################################## IF NO DIFFERENCED - SUCCESS !!! ###################################################






############################# CHECKING HOW MANY PATIENTS ARE IN mysql ##################################################################################

select a.ldap_org_id, b.org_name, format(count(*),0) as count from (select distinct c.ldap_org_id, c.apixio_id, c.status from apixiomain.act_patient c where c.status = 'active') a, apixiomain.ldap_org b where (a.ldap_org_id = b.ldap_org_id) group by a.ldap_org_id;


Filter by specific ORG_ID.

########################################################################################################################################################




============================ NEW PIPELINE BUILD STAGE CLUSTER =======================

apixio-pipeline

apixio-pipeline-trigger-stage



============================ SAVED 13,115 ZIPPED ENCRYPTED FILES =============================================

hadoop fs -mkdir /user/apxqueue/saved/13115MMG

hadoop fs -cp /user/apxqueue/uploads/igor34testorg/igor34sourcesystem/P1/* /user/apxqueue/saved/13115MMG/

hadoop fs -cp /user/apxqueue/saved/13115MMG/* /user/apxqueue/uploads/igor34testorg/igor34sourcesystem/P1/


hadoop fs -mkdir /user/apxqueue/saved/13106MMG

hadoop fs -cp /user/apxqueue/uploads/igor39testorg/igor39sourcesystem/P1/* /user/apxqueue/saved/13106MMG/

hadoop fs -cp /user/apxqueue/saved/13106MMG/* /user/apxqueue/uploads/igor39testorg/igor39sourcesystem/P1/

hadoop fs -cp /user/apxqueue/saved/13106MMG/* /user/apxqueue/uploads/org429/sourcesystem/P1

hadoop fs -cp /user/apxqueue/pipeline-igor2/process/* /user/apxqueue/uploads/org429/sourcesystem/P1





hadoop fs -mv /user/apxqueue/pipeline-igor1/process/* /user/apxqueue/uploads/gwu/P2

hadoop fs -mv /user/apxqueue/pipeline-igor1/largefile/* /user/apxqueue/uploads/gwu/P2
hadoop fs -mv /user/apxqueue/pipeline-igor1/error/* /user/apxqueue/uploads/gwu/P2

hadoop fs -cp /user/apxqueue/pipeline-igor1/staging/temp_queue_337.2247654084779_1358553900006/* /user/apxqueue/uploads/gwu/P2

hadoop fs -mv /user/apxqueue/pipeline-igor1/processing/temp_342.00822838760524_1359064560022/* /user/apxqueue/pipeline-igor1/queue

------------------------------------------------------------------------------------------------

hadoop fs -cp /user/apxqueue/uploads/ocr/p1/* /user/apxqueue/saved/nethra/

hadoop fs -cp /user/apxqueue/saved/nethra/* /user/apxqueue/uploads/org429/sourcesystem/P1/

hadoop fs -cp /user/apxqueue/pipeline-igor2/queue/* /user/apxqueue/saved/nethra/

==============================================================================================================
========================== PIPELINE RELEASE DATA LOCATION AND FILES INVENTORY ================================
==============================================================================================================

1.) RUN data.bat command to copy files from original location into test-ready folder for Indexer [Source]
2.) Point Indexer to the following SOURCE filder [


ORGANIZATION A

CCR:   \\10.19.220.94\pipeline-test-data\OrgA\CCR\
\201302080426440782 
(500 files)

Document catalog files:   \\10.19.220.94\pipeline-test-data\OrgA\Doc_Catalog\
\201302080348307643 (1,000 files)
\201302080349000483 (1,001 files)
\201302080349238319 (1,001 files)
\201302080349470060 (1,001 files)
\201302080350124303 (1,001 files)
\201302080350438864 (1,001 files)
\201302080351082169 (1,001 files)
\201302080351321879 (511 fles)
(7,517 files)

Image catalog files:   \\10.19.220.94\pipeline-test-data\OrgA\Img_Catalog\
\201302080402555035 (1,000 files)
\201302080403195996 (1,001 files)
\201302080403351480 (1,001 files)
\201302080403497431 (1,001 files)
\201302080404056665 (1,001 files)
\201302080404231057 (1,001 files)
\201302080404400917 (1,001 files)
\201302080404558588 (448 files)
(7,454 files)

Documents:   \\10.19.220.94\pipeline-test-data\OrgA\Doc\
(7,517 files)

Images:   \\10.19.220.94\pipeline-test-data\OrgA\Img\
(16,335 files)

TOTAL: 15,471


--------------------------------------------------------------------------------------------------------------

ORGANIZATION B

CCR:   \\10.19.220.94\pipeline-test-data\OrgB\CCR\
\201302080731310955 (500 files)

Document catalog files:   \\10.19.220.94\pipeline-test-data\OrgB\Doc_Catalog\
\201302080322005648 (1,000 files)
\201302080322431001 (1,001 files)
\201302080323206662 (1,001 files)
\201302080324056861 (1,001 files)
\201302080324397675 (1,001 files)
\201302080325148959 (1,001 files) 
\201302080325483679 (1,001 files)
\201302080326261684 (337 files)
(7,343 files)

Image catalog files:   \\10.19.220.94\pipeline-test-data\OrgB\Img_Catalog\
\201302080405469419 (1,000 files)
\201302080405597712 (1,001 files)
\201302080406158509 (1,001 files)
\201302080406325712 (1,001 files)
\201302080406480415 (1,001 files)
\201302080407009333 (1,001 files)
\201302080407149503 (479 files)
(6,484 files)

Documents:   \\10.19.220.94\pipeline-test-data\OrgB\Doc\
(7,343 files)

Images:   \\10.19.220.94\pipeline-test-data\OrgB\Img\
(13,688 files)

TOTAL: 14,327

=======================================RUN.SH PIG SCRIPT========================================================

export PIG_INITIAL_ADDRESS=184.169.169.1
export PIG_RPC_PORT=9160
export PIG_PARTITIONER='org.apache.cassandra.dht.RandomPartitioner'
pig -Dmapred.job.queue.name=fastlane -Dmapreduce.map.env="PIG_INITIAL_ADDRESS=184.169.169.1,PIG_RPC_PORT=9160,
PIG_PARTITIONER=org.apache.cassandra.dht.RandomPartitioner" -Dmapred.map.child.java.opts="-Xmx2000m" -Dmapred.map.child.env="PIG_INITIAL_ADDRESS=184.169.169.1,PIG_RPC_PORT=9160,PIG_PARTITIONER=org.apache.cassandra.dht.RandomPartitioner" -Dcassandra.range.size=10 -Dcassandra.input.split.size=200 -Dmapred.compress.map.output="false" -Dmapred.map.out.compress="false" CombineLabDocEncryptionUserPassWithSearch.pig

=================================================================================================================
================================== DEMO SERVER ==================================================================
so you can get to it at demo.apixio.com
and the ip address to point the pipeline at is 10.188.157.166
i can create the column families for you if you like
8p1dem19
can ssh directly without key

External ip:  demo.apixio.com  (50.18.252.12)
Internal ip which hadoop connects to:   10.188.157.166

Username:  hit
Password:  will send over skype
Password: 8p1sql19

MySQL username and password is same as for staging cluster.

ip: 10.188.157.166
user: apxDB
pw: apxMySQL

======================================= SAVING MMG 2-ORG DATA =================================================

copy c:\_Pipeline\Indexer\WORK\transmitted\*.* c:\_Pipeline\Indexer\SAVED\transmitted\
copy c:\_Pipeline\Indexer2\WORK\transmitted\*.* c:\_Pipeline\Indexer2\SAVED\transmitted\

hadoop fs -count /user/apxqueue/uploads/apxdemot703/sourcesystem/P1
hadoop fs -count /user/apxqueue/uploads/apxdemot704/sourcesystem/P1

hadoop fs -mkdir /user/apxqueue/saved/ORG1MMG15430
hadoop fs -mkdir /user/apxqueue/saved/ORG2MMG14301
hadoop fs -mkdir /user/apxqueue/saved/MMG10SEQ

hadoop fs -cp /user/apxqueue/uploads/apxdemot713/sourcesystem/P1/* /user/apxqueue/saved/ORG1MMG15430/
hadoop fs -cp /user/apxqueue/uploads/apxdemot714/sourcesystem/P1/* /user/apxqueue/saved/ORG2MMG14301/

hadoop fs -mkdir /user/apxqueue/uploads/apxdemot717/sourcesystem/P1/
hadoop fs -cp /user/apxqueue/saved/ORG1MMG15430/* /user/apxqueue/uploads/apxdemot717/sourcesystem/P1/
hadoop fs -mkdir /user/apxqueue/uploads/apxdemot716/sourcesystem/P1/
hadoop fs -cp /user/apxqueue/saved/ORG2MMG14301/* /user/apxqueue/uploads/apxdemot716/sourcesystem/P1/

hadoop fs -copyFromLocal /home/apxqueue/igor-automation/ORG1_Manifest/1362167930635.zip /user/apxqueue/indexer_logs_713/apxdemot713/sourcesystem/
hadoop fs -copyFromLocal /home/apxqueue/igor-automation/ORG2_Manifest/1362104725599.zip /user/apxqueue/indexer_logs_713/apxdemot714/sourcesystem/

========== copy and save sequence files for apxdemot713 for future testing ===============================


hadoop fs -cp /user/apxqueue/pipeline-igor1/general/p1/queue/* /user/apxqueue/saved/MMG10SEQ


======================================= AUTOMATED QCTOOL ====================================================

STAGE_CLUSTER

/home/apxqueue/igor-automation

./qc_automation.sh


/user/apxqueue/indexer_logs_729/apxdemod729/validations/validation_report_1363991330


/user/apxqueue/indexer_logs_729/apxdemod729/validations/validation_report_1363991330/cassandra_validation
/user/apxqueue/indexer_logs_729/apxdemod729/validations/validation_report_1363991330/missing_files
/user/apxqueue/indexer_logs_729/apxdemod729/validations/validation_report_1363991330/pipeline_validation

cassandra_validation - difference (missing documents in Cassandra)
missing_files - (missing files in both Cassandra and Pipeline folders)
pipeline_validation - (missing files in pipeline folders)

=============================================================================================================

hadoop fs -cp /user/apxqueue/pipeline-igor3/general/p1/success/* /user/apxqueue/uploads/apxdemot760/sourcesystem/P1/

SAVED 1 SEQ file with 3 orgs, apxdemot761, 762 and 763. 10,20 and 30 docs.

hadoop fs -cp /user/apxqueue/pipeline-igor1/general/p1/queue/* /user/apxqueue/saved/3ORG_SEQ

hadoop fs -cp /user/apxqueue/saved/3ORG_SEQ/* /user/apxqueue/pipeline-igor1/general/p1/queue

==================================================================================================================
================== 2 ORG APXDEMOT871 AND APXDEMOT872 MMG 211 50MB SAVED SEQUENCE FILES ===========================
==================================================================================================================

hadoop fs -cp /user/apxqueue/pipeline-igor25/general/p1/queue/* /user/apxqueue/saved/APXDEMOT87187250MB211SEQFLS/

========================================= NETHRA =================================================================

delete from apixiomain.act where ldap_org_id=10000814;
delete from apixiomain.act_patient where ldap_org_id=10000814;
truncate cf10000814;
that's for cassandra clearing..
so next time if you wanna reuse an existign acccount n u wanna clean it, you can run these 3 commands..
to login to cassandra.. just type cass.sh
in stage machine..
then say use apixio;
"use apixio;"



[4:50:47 PM] Nethra Krishna: cassandra manifest program is in this jar file: /home/apxqueue/nethra/patientmanifest/apixio-cassandra-utils-1.0.6-SNAPSHOT.jar
[4:51:53 PM] Nethra Krishna: to run this jar, the cmd is: hadoop jar apixio-cassandra-utils-1.0.6-SNAPSHOT.jar -files apixio-security.properties `cassandra ip` `column family` `output folder`
[4:52:08 PM] Nethra Krishna: oh, in the same folder as the jar, plz put apixio-security.properties
yeah.. then compare scripts.. in in nethra/automation/compare_manifests_nethra.pig
[4:53:24 PM] Nethra Krishna: run script for that pig script is in : run_validation.sh

to run pig script, then say : ./run_validation.sh `location of indexer manifest` `location of cassandra manifest` `output folder`

================================== CASSANDRA MANIFEST CREATION ===================================================================================================

hadoop jar apixio-cassandra-utils-1.0.6-SNAPSHOT.jar -files apixio-security.properties 184.169.168.150 cf10000816 /user/apxqueue/pipeline-igor24/general/p1/manifest


================================== QC TOOL COMPARISON ============================================================================================================

nethra.automation: compare_manifests_nethra.pig, run_comparison.sh
we need those two..

./run_comparison.sh /user/apxqueue/pipeline-igor24/general/p1/indmanifest/ /user/apxqueue/pipeline-igor24/general/p1/manifest/ /user/apxqueue/pipeline-igor24/general/p1/manifest/output

========================================= NETHRA =================================================================================================================
=========================================================== CLEANING UP CASSANDRA AND MYSQL ======================================================================
========= CASSANDRA =============
cass.sh
use apixio;
truncate `columnfamily`;
exit;

========= MY SQL =================
delete from apixiomain.act where ldap_org_id=10000816;
delete from apixiomain.act_patient where ldap_org_id=10000816;
==================================================================================================================================================================

TESTING STEPS - CASSANDRA - FOR NETHRA'S EXT ID's TEST

cass.sh
use apixio;
assume cf10000828 keys as ascii;
get cf10000828['pat:26907608-4489-46e7-85c7-bfa889e06a94'];




get cf10000827['pat:8430e8ed-ff2f-4265-a667-82e88c5dea99'];
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'pat:8430e8ed-ff2f-4265-a667-82e88c5dea99' as hex bytes

help assume;
assume <cf> comparator as <type>;
assume <cf> sub_comparator as <type>;
assume <cf> validator as <type>;
assume <cf> keys as <type>;

Assume one of the attributes (comparator, sub_comparator, validator or keys)
of the given column family match specified type. The specified type will
be used when displaying data returned from the column family.

This statement does not change the column family definition stored in
Cassandra. It only affects the cli and how it will transform values
to be sent to and interprets results from Cassandra.

If results from Cassandra do not validate according to the assumptions an
error is displayed in the cli.

Required Parameters:
- cf: Name of the column family to make the assumption about.

- type: Validator type to use when processing values.

  Supported values are:
    - ascii
    - bytes
    - counterColumn (distributed counter column)
    - int
    - integer (a generic variable-length integer type)
    - lexicalUUID
    - long
    - utf8

  It is also valid to specify the fully-qualified class name to a class that
  extends org.apache.cassandra.db.marshal.AbstractType.

Examples:
assume Standard1 comparator as lexicaluuid;
assume Standard1 keys as ascii;

assume cf10000827 keys as ascii;
Assumption for column family 'cf10000827' added successfully.

assume cf10000827 sub_comparator as ascii;
Assumption for column family 'cf10000827' added successfully.

get cf10000827['pat:8430e8ed-ff2f-4265-a667-82e88c5dea99'];
=> (column=part:2310cfd18f806eacd25fb8349257e28e, value=31333736363935303738373631, timestamp=1376695103523)
=> (column=part:aa36ef1d23c2da0e3af56b5ffcbf6750, value=31333736363935303739303332, timestamp=1376695103523)
Returned 2 results.
Elapsed time: 22 msec(s).

====================================== testing new pipeline - onlok data from Alex ======================================
/user/apxqueue/saved/ONLOK (backup folder)

/user/apxqueue/uploads/onlok_indexer@apixio.com/NextGen_bak/P1


hdfs://10.176.235.204:8020

/user/apxqueue/pipeline-igor38/general/p1/processing/temp_802.6699582418308_1377044880014 (bad sequence file)




=========================================================================================================================


./run_comparison.sh /user/apxqueue/pipeline-igor46/general/p1/output/indexer1/apxdemot908/ /user/apxqueue/pipeline-igor46/general/p1/output/apxdemot908/
 
/user/apxqueue/pipeline-igor46/general/p1/manifest/output1


./run_comparison.sh /user/apxqueue/pipeline-igor46/general/p1/output/indexer1/apxdemot909/ /user/apxqueue/pipeline-igor46/general/p1/output/apxdemot909/
 
/user/apxqueue/pipeline-igor46/general/p1/manifest/output2

-----------------------------------------------------------------------------------------------------------------------------


hadoop jar apixio-cassandra-utils-1.0.6-SNAPSHOT.jar removedocs -files apixio-security.properties,docIdFilter.txt 184.169.139.118 cf10000833

/user/apxqueue/pipeline-igor41/general/p1/removedfiles


hadoop jar apixio-cassandra-utils-1.0.6-SNAPSHOT.jar -files apixio-security.properties 
184.169.139.118 cf10000830 /user/apxqueue/pipeline-igor46/general/p1/output/apxdemot908/

hadoop jar apixio-cassandra-utils-1.0.6-SNAPSHOT.jar -files apixio-security.properties 
184.169.139.118 cf10000831 /user/apxqueue/pipeline-igor46/general/p1/output/apxdemot9091/ - new

hadoop jar apixio-cassandra-utils-1.0.6-SNAPSHOT.jar -files apixio-security.properties 

10.196.73.186 cf15 /user/apxqueue/pipeline-igor46/general/p1/output/apxdemot0006/




/user/apxqueue/pipeline-igor46/general/p1/output/apxdemot908/ - cassandra
/user/apxqueue/pipeline-igor46/general/p1/output/comparison/apxdemot908/
/user/apxqueue/pipeline-igor46/general/p1/output/indexer/apxdemot908/ - indexer
/user/apxqueue/pipeline-igor46/general/p1/output/indexer1/apxdemot908/ - indexer


/user/apxqueue/pipeline-igor46/general/p1/output/apxdemot909/ - cassandra
/user/apxqueue/pipeline-igor46/general/p1/output/comparison/apxdemot909/
/user/apxqueue/pipeline-igor46/general/p1/output/indexer/apxdemot909/ - indexer
/user/apxqueue/pipeline-igor46/general/p1/output/indexer1/apxdemot909/



/user/apxqueue/indexer_logs/apxdemot908/sourcesystem/
/user/apxqueue/indexer_logs/apxdemot909/sourcesystem/
