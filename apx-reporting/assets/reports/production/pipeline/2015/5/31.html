 <h1>Apixio Daily Pipeline QA Report</h1>
Date & Time (run): <b>06/01/2015 17:13:09</b><br>
Date (logs & queries): <b>"05"/"31"/2015</b><br>
Report type: <b>Daily engineering QA</b><br>
Enviromnent: <b><font color='red'>Production</font></b><br><br>
<table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;FAILED JOBS</b></font></td></tr></table><table border='1' width='800' cellspacing='0'><tr><td>Activity:</td><td>Hadoop job:</td><td>Batch ID:</td><td>Organization:</td><td>Failure Time:</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44750&nbsp;&nbsp;</td> 			<td>10000291-page-migration&nbsp;&nbsp;</td><td>HealthCare Partners (10000291)</td><td>May 31 06:56 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44748&nbsp;&nbsp;</td> 			<td>10000291-page-migration&nbsp;&nbsp;</td><td>HealthCare Partners (10000291)</td><td>May 31 06:54 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44745&nbsp;&nbsp;</td> 			<td>10000291-page-migration&nbsp;&nbsp;</td><td>HealthCare Partners (10000291)</td><td>May 31 06:51 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44733&nbsp;&nbsp;</td> 			<td>10000291-page-migration&nbsp;&nbsp;</td><td>HealthCare Partners (10000291)</td><td>May 31 06:17 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44752&nbsp;&nbsp;</td> 			<td>10000291-page-migration&nbsp;&nbsp;</td><td>HealthCare Partners (10000291)</td><td>May 31 07:04 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44668&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 02:11 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44682&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 06:59 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44722&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 06:55 AM</td></tr><tr> 			<td>qaPage&nbsp;&nbsp;</td> 			<td>job_1430780805219_44739&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 06:55 AM</td></tr><tr> 			<td>qaPage&nbsp;&nbsp;</td> 			<td>job_1430780805219_44737&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 06:53 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44541&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 06:32 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44708&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 06:31 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44697&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 06:21 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44731&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 06:04 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44713&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 02:58 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44717&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 02:58 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44709&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 02:29 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44543&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 07:17 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44696&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 07:08 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44714&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 03:09 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44680&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 03:06 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44684&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 01:51 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44707&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 01:45 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44706&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 01:38 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44665&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 01:03 AM</td></tr><tr> 			<td>pager&nbsp;&nbsp;</td> 			<td>job_1430780805219_44646&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 01:02 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44728&nbsp;&nbsp;</td> 			<td>10000327-page-migration&nbsp;&nbsp;</td><td>Wellpoint (10000327)</td><td>May 31 05:44 AM</td></tr><tr> 			<td>qaPage&nbsp;&nbsp;</td> 			<td>job_1430780805219_44740&nbsp;&nbsp;</td> 			<td>10000330-page-migration&nbsp;&nbsp;</td><td>Wellcare (10000330)</td><td>May 31 06:51 AM</td></tr><tr> 			<td>qaPage&nbsp;&nbsp;</td> 			<td>job_1430780805219_44732&nbsp;&nbsp;</td> 			<td>10000330-page-migration&nbsp;&nbsp;</td><td>Wellcare (10000330)</td><td>May 31 06:18 AM</td></tr><tr> 			<td>qaPage&nbsp;&nbsp;</td> 			<td>job_1430780805219_44716&nbsp;&nbsp;</td> 			<td>10000330-page-migration&nbsp;&nbsp;</td><td>Wellcare (10000330)</td><td>May 31 02:59 AM</td></tr><tr> 			<td>qaPage&nbsp;&nbsp;</td> 			<td>job_1430780805219_44749&nbsp;&nbsp;</td> 			<td>10000331-page-migration&nbsp;&nbsp;</td><td>UAM (10000331)</td><td>May 31 07:09 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44747&nbsp;&nbsp;</td> 			<td>10000336-page-migration&nbsp;&nbsp;</td><td>NTSP (10000336)</td><td>May 31 06:55 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44744&nbsp;&nbsp;</td> 			<td>10000336-page-migration&nbsp;&nbsp;</td><td>NTSP (10000336)</td><td>May 31 06:51 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44753&nbsp;&nbsp;</td> 			<td>10000336-page-migration&nbsp;&nbsp;</td><td>NTSP (10000336)</td><td>May 31 07:04 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44756&nbsp;&nbsp;</td> 			<td>10000336-page-migration&nbsp;&nbsp;</td><td>NTSP (10000336)</td><td>May 31 07:04 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44760&nbsp;&nbsp;</td> 			<td>10000336-page-migration&nbsp;&nbsp;</td><td>NTSP (10000336)</td><td>May 31 07:07 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44759&nbsp;&nbsp;</td> 			<td>10000336-page-migration&nbsp;&nbsp;</td><td>NTSP (10000336)</td><td>May 31 07:07 AM</td></tr><tr> 			<td>pagePersist&nbsp;&nbsp;</td> 			<td>job_1430780805219_44758&nbsp;&nbsp;</td> 			<td>10000336-page-migration&nbsp;&nbsp;</td><td>NTSP (10000336)</td><td>May 31 07:07 AM</td></tr></table><table><tr><td bgcolor='#DF1000' align='center' width='800'><font size='3' color='white'><b>STATUS - FAILED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;SPECIFIC ERRORS</b></font></td></tr></table><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>DR</b> summary_docreceiver_upload specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>DR</b> summary_docreceiver_archive specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>DR</b> summary_docreceiver_seqfile specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>Parser</b> summary_parser specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>OCR</b> summary_ocr specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>Persist Mapper</b> summary_persist_mapper specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>Persist Reducer</b> summary_persist_reducer specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>Event Mapper</b> summary_event_mapper specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>Event Reducer</b> summary_event_reducer specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>Load APO</b> summary_loadapo specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td colspan='4'>There were no <b>HCC</b> summary_hcc_error specific errors</td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>485</td><td bgcolor='#FFFF00'>Highmark (10000328)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>296</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>277</td><td bgcolor='#FFFF00'>Wellcare (10000330)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>135</td><td bgcolor='#FFFF00'>Cambia (10000318)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>58</td><td bgcolor='#FFFF00'>UAM (10000331)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>52</td><td bgcolor='#FFFF00'>HealthCare Partners (10000291)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>10</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>3</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.0.2.209:50010], original=[10.0.2.209:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration./	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:981)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1047)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1194)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:945)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:496)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>2</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at com.apixio.security.Security.encryptBytesToBytes(Security.java:428)/	at com.apixio.security.FileEncryptor.encryptInMemory(FileEncryptor.java:558)/	at com.apixio.dao.blobdao.BlobDAO.write(BlobDAO.java:39)/	at com.apixio.jobs.image.pages.mapper.PagePersistMapper.map(PagePersistMapper.java:99)/	at com.apixio.jobs.image.pages.mapper.PagePersistMapper.map(PagePersistMapper.java:24)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>2</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080093/output/_temporary/1/_temporary/attempt_1430780805219_44758_m_000001_0/qaPageOutput/qaPageOutput-m-00001-1433056018687 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>2</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>2</td><td bgcolor='#FFFF00'>Network Health HCC (10000337)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080055/output/_temporary/1/_temporary/attempt_1430780805219_44753_m_000001_0/qaPageOutput/qaPageOutput-m-00001-1433055818080 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at com.apixio.security.Security.decryptBytesToBytes(Security.java:462)/	at com.apixio.jobs.image.pages.mapper.PagePersistMapper.map(PagePersistMapper.java:92)/	at com.apixio.jobs.image.pages.mapper.PagePersistMapper.map(PagePersistMapper.java:24)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080079/output/_temporary/1/_temporary/attempt_1430780805219_44756_m_000001_0/qaPageOutput/qaPageOutput-m-00001-1433055837864 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080177/output/_temporary/1/_temporary/attempt_1430780805219_44706_m_000003_0/qaPageOutput/qaPageOutput-m-00003-1433036162562 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080199/output/_temporary/1/_temporary/attempt_1430780805219_44728_m_000001_0/qaPageOutput/qaPageOutput-m-00001-1433051027917 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>HealthCare Partners (10000291)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3079881/output/_temporary/1/_temporary/attempt_1430780805219_44733_m_000000_0/qaPageOutput/qaPageOutput-m-00000-1433053017477 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at com.apixio.security.Security.decryptBytesToBytes(Security.java:457)/	at com.apixio.jobs.image.pages.mapper.PagePersistMapper.map(PagePersistMapper.java:92)/	at com.apixio.jobs.image.pages.mapper.PagePersistMapper.map(PagePersistMapper.java:24)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080181/output/_temporary/1/_temporary/attempt_1430780805219_44709_m_000002_0/qaPageOutput/qaPageOutput-m-00002-1433038282253 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3079941/output/_temporary/1/_temporary/attempt_1430780805219_44747_m_000000_0/qaPageOutput/qaPageOutput-m-00000-1433055224194 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3079901/output/_temporary/1/_temporary/attempt_1430780805219_44744_m_000000_0/qaPageOutput/qaPageOutput-m-00000-1433055025925 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080125/output/_temporary/1/_temporary/attempt_1430780805219_44759_m_000002_0/qaPageOutput/qaPageOutput-m-00002-1433056018716 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080205/output/_temporary/1/_temporary/attempt_1430780805219_44731_m_000000_0/qaPageOutput/qaPageOutput-m-00000-1433051634463 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080179/output/_temporary/1/_temporary/attempt_1430780805219_44707_m_000001_0/qaPageOutput/qaPageOutput-m-00001-1433035982244 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3080139/output/_temporary/1/_temporary/attempt_1430780805219_44760_m_000002_0/qaPageOutput/qaPageOutput-m-00002-1433056032856 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_page_persist</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>HealthCare Partners (10000291)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3079987/output/_temporary/1/_temporary/attempt_1430780805219_44752_m_000006_0/qaPageOutput/qaPageOutput-m-00006-1433055817870 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>578</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.0.2.210:50010], original=[10.0.2.210:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration./	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:981)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1047)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1194)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:945)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:496)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>256</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.io.IOException: All datanodes 10.0.2.213:50010 are bad. Aborting.../	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1147)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:945)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:496)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>66</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3078273/output/_temporary/1/_temporary/attempt_1430780805219_44697_m_000000_0/documentPages/documentPages-m-00000-1433024099399 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>58</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.0.2.213:50010], original=[10.0.2.213:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration./	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:981)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1047)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1194)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:945)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:496)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>47</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.0.2.211:50010], original=[10.0.2.211:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration./	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:981)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1047)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1194)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:945)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:496)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>25</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>Status Code: 404, AWS Service: Amazon S3, AWS Error Message: The specified key does not exist. / java.lang.ArrayIndexOutOfBoundsException</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>4</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at java.util.Arrays.copyOf(Arrays.java:2271)/	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)/	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)/	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.InputStreamToArray(Unknown Source)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at net.sf.ghost4j.document.PDFDocument.getPageCount(PDFDocument.java:79)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:119)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:32)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>3</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at java.util.Arrays.copyOf(Arrays.java:2271)/	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)/	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)/	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.InputStreamToArray(Unknown Source)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at net.sf.ghost4j.document.PDFDocument.getPageCount(PDFDocument.java:79)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:103)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:32)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>2</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at java.util.Arrays.copyOf(Arrays.java:2271)/	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)/	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)/	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.InputStreamToArray(Unknown Source)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at net.sf.ghost4j.document.PDFDocument.getPageCount(PDFDocument.java:79)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:103)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:32)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>2</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at java.util.Arrays.copyOf(Arrays.java:2271)/	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)/	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)/	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.InputStreamToArray(Unknown Source)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at net.sf.ghost4j.document.PDFDocument.getPageCount(PDFDocument.java:79)/	at net.sf.ghost4j.renderer.AbstractRemoteRenderer.render(AbstractRemoteRenderer.java:72)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:122)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:32)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3078257/output/_temporary/2/_temporary/attempt_1430780805219_44682_m_000000_1000/documentPages/documentPages-m-00000-1433023475795 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at java.util.Arrays.copyOf(Arrays.java:2271)/	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)/	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)/	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.InputStreamToArray(Unknown Source)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at net.sf.ghost4j.document.PDFDocument.load(PDFDocument.java:38)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:102)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:32)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3078271/output/_temporary/1/_temporary/attempt_1430780805219_44696_m_000002_0/documentPages/documentPages-m-00002-1433053985850 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)/	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3078309/output/_temporary/1/_temporary/attempt_1430780805219_44722_m_000000_0/documentPages/documentPages-m-00000-1433043554898 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at java.util.Arrays.copyOf(Arrays.java:2271)/	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)/	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)/	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)/	at net.sf.ghost4j.document.AbstractDocument.load(AbstractDocument.java:60)/	at net.sf.ghost4j.document.PDFDocument.load(PDFDocument.java:30)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:102)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:32)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>java.lang.OutOfMemoryError: Java heap space/	at java.util.Arrays.copyOf(Arrays.java:2271)/	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)/	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)/	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.InputStreamToArray(Unknown Source)/	at com.lowagie.text.pdf.RandomAccessFileOrArray.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at com.lowagie.text.pdf.PdfReader.<init>(Unknown Source)/	at net.sf.ghost4j.document.PDFDocument.load(PDFDocument.java:38)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:102)/	at com.apixio.jobs.image.pages.mapper.PagerMapper.map(PagerMapper.java:32)/	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)/	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)/	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)/	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)/</i></td></tr><tr><td bgcolor='#FFFF00'><b>Page Extraction</b> summary_pager</td><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td colspan='4' bgcolor='#FFFF00'>Error: <i>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/apxqueue/queue-location-1/jobs/3078207/output/_temporary/1/_temporary/attempt_1430780805219_44543_m_000000_1/documentPages/documentPages-m-00000-1433021838513 could only be replicated to 0 nodes instead of minReplication (=1).  There are 7 datanode(s) running and no node(s) are excluded in this operation./	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1489)/	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3066)/	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)/	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)/	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)/	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)/	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)/	at java.security.AccessController.doPrivileged(Native Method)/	at javax.security.auth.Subject.doAs(Subject.java:415)/	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)/	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)//	at org.apache.hadoop.ipc.Client.call(Client.java:1411)/	at org.apache.hadoop.ipc.Client.call(Client.java:1364)/	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)/	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)/	at sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)/	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)/	at java.lang.reflect.Method.invoke(Method.java:606)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)/	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)/	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)/	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)/</i></td></tr></table><br><table><tr><td bgcolor='#DF1000' align='center' width='800'><font size='3' color='white'><b>STATUS - FAILED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;UPLOAD SUMMARY</b></font></td></tr></table><table border='1' width='800' cellspacing='0'><tr><td>Activity:</td><td>Doc Count:</td><td>Status:</td><td>Organization:</td></tr><tr><td colspan='5'><i>There were no Doc-Receiver summary_docreceiver_upload errors</i></td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td>Activity:</td><td>Doc Count:</td><td>Status:</td><td>Organization:</td></tr><tr><td colspan='5'><i>There were no OCR summary_ocr errors</i></td></tr></table><br><table border='1' width='800' cellspacing='0'><tr><td>Activity:</td><td>Doc Count:</td><td>Status:</td><td>Organization:</td></tr><tr><td colspan='5'><i>There were no Persist Mapper summary_persist_mapper errors</i></td></tr></table><br><table><tr><td bgcolor='#00A303' align='center' width='800'><font size='3' color='white'><b>STATUS - PASSED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;JOB SUMMARY</b></font></td></tr></table><table border='1' width='800' cellspacing='0'><tr><td>Count:</td><td>Status:</td><td>Activity:</td><td>Organization:</td></tr><tr><td>6</td><td>success</td><td>pagePersist</td><td>HealthCare Partners (10000291)</td></tr><tr><td bgcolor='#FFFF00'>5</td><td bgcolor='#FFFF00'>error</td><td bgcolor='#FFFF00'>pagePersist</td><td bgcolor='#FFFF00'>HealthCare Partners (10000291)</td></tr><tr><td>14</td><td>success</td><td>qaPage</td><td>HealthCare Partners (10000291)</td></tr><tr><td>6</td><td>success</td><td>qaPage</td><td>Cambia (10000318)</td></tr><tr><td bgcolor='#FFFF00'>7</td><td bgcolor='#FFFF00'>error</td><td bgcolor='#FFFF00'>pagePersist</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td>6</td><td>success</td><td>pager</td><td>Wellpoint (10000327)</td></tr><tr><td bgcolor='#FFFF00'>13</td><td bgcolor='#FFFF00'>error</td><td bgcolor='#FFFF00'>pager</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td>4</td><td>success</td><td>qaPage</td><td>Wellpoint (10000327)</td></tr><tr><td bgcolor='#FFFF00'>2</td><td bgcolor='#FFFF00'>error</td><td bgcolor='#FFFF00'>qaPage</td><td bgcolor='#FFFF00'>Wellpoint (10000327)</td></tr><tr><td>1</td><td>success</td><td>pagePersist</td><td>Highmark (10000328)</td></tr><tr><td>10</td><td>success</td><td>qaPage</td><td>Highmark (10000328)</td></tr><tr><td>1</td><td>success</td><td>pagePersist</td><td>Wellcare (10000330)</td></tr><tr><td>1</td><td>success</td><td>pager</td><td>Wellcare (10000330)</td></tr><tr><td>9</td><td>success</td><td>qaPage</td><td>Wellcare (10000330)</td></tr><tr><td bgcolor='#FFFF00'>3</td><td bgcolor='#FFFF00'>error</td><td bgcolor='#FFFF00'>qaPage</td><td bgcolor='#FFFF00'>Wellcare (10000330)</td></tr><tr><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>error</td><td bgcolor='#FFFF00'>qaPage</td><td bgcolor='#FFFF00'>UAM (10000331)</td></tr><tr><td>3</td><td>success</td><td>qaPage</td><td>UAM (10000331)</td></tr><tr><td bgcolor='#FFFF00'>7</td><td bgcolor='#FFFF00'>error</td><td bgcolor='#FFFF00'>pagePersist</td><td bgcolor='#FFFF00'>NTSP (10000336)</td></tr><tr><td>5</td><td>success</td><td>pagePersist</td><td>NTSP (10000336)</td></tr><tr><td>4</td><td>success</td><td>pager</td><td>NTSP (10000336)</td></tr><tr><td>5</td><td>success</td><td>qaPage</td><td>NTSP (10000336)</td></tr><tr><td>1</td><td>success</td><td>qaPage</td><td>Network Health HCC (10000337)</td></tr><tr><td colspan='4' align='left' bgcolor='#D0D0D0'><b> 		114</b> - Total number of Jobs processed, out of which <font color='#DF1000'><b>38 failed</b></font> and <font color='#00A303'><b>76 succeeded</b></font> 		</font></td></tr></table><br><table><tr><td bgcolor='#DF1000' align='center' width='800'><font size='3' color='white'><b>STATUS - FAILED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;CARE OPTIMIZER</b></font></td></tr></table><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Error(s) summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td width='5%'>Count:</td><td width='75%'>Message:</td><td width='10%'>1st Occur:</td><td width='10%'>Last Occur:</td></tr><tr><td align='center' colspan='4'><i>Logs data is missing</i></td></tr></table><br><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Load summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td># Loads:</td><td>Organization:</td><td>Av Load sec:</td><td>Max Load sec:</td><td>5% sec:</td><td>50% sec:</td><td>95% sec:</td><td>Av Patient:</td><td>Max Patient</td><td>Av Mb/Sec:</td><td>Min Pat Cache:</td><td>Max Pat Cache:</td></tr><tr><td align='center' colspan='12'><i>Logs data is missing</i></td></tr></table><br><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Search summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Organization:</td><td># Users:</td><td># Pat:</td><td>Min mil:</td><td>Max mil:</td><td>Av mil:</td><td>5% mil:</td><td>50% mil:</td><td>95% mil:</td><td>1st acc:</td><td>Lst acc:</td></tr><tr><td align='center' colspan='11'><i>Logs data is missing</i></td></tr></table><br><table><tr><td bgcolor='#00A303' align='center' width='800'><font size='3' color='white'><b>STATUS - PASSED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;LOGS TRAFFIC</b></font></td></tr></table><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Logstraffic summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Application:</td><td>Discarded:</td><td>Infos:</td><td>Events:</td><td>Errors:</td><td>Total:</td></tr><tr><td bgcolor=#FFFFFF>hcc</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>9777</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>9777</td></tr><tr><td bgcolor=#FFFF00>coordinator</td><td bgcolor=#FFFF00>0</td><td bgcolor=#FFFF00>0</td><td bgcolor=#FFFF00>1798</td><td bgcolor=#FFFF00>653</td><td bgcolor=#FFFF00>2451</td></tr><tr><td bgcolor=#FFFFFF>careopt</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>1455</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>1455</td></tr><tr><td bgcolor=#FFFFFF>dataorchestrator</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>1440</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>1440</td></tr><tr><td bgcolor=#FFFFFF>useraccount</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>737</td><td bgcolor=#FFFFFF>0</td><td bgcolor=#FFFFFF>737</td></tr></table><br><table><tr><td bgcolor='#00A303' align='center' width='800'><font size='3' color='white'><b>STATUS - PASSED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;EVENTS</b></font></td></tr></table><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Table: summary_event_address</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Event count:</td><td>Error message:</td><td>Status:</td><td>Org(ID):</td></tr><tr><td align='center' colspan='4'><i>Logs data is missing</i></td></tr></table><br><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Table: summary_event_mapper</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Event count:</td><td>Error message:</td><td>Status:</td><td>Org(ID):</td></tr><tr><td align='center' colspan='4'><i>Logs data is missing</i></td></tr></table><br><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Table: summary_event_reducer</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Event count:</td><td>Error message:</td><td>Status:</td><td>Org(ID):</td></tr><tr><td align='center' colspan='4'><i>Logs data is missing</i></td></tr></table><br><table><tr><td bgcolor='#00A303' align='center' width='800'><font size='3' color='white'><b>STATUS - PASSED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;DATA ORCHESTRATOR</b></font></td></tr></table><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>ACL(s) summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Count:</td><td>Permission:</td><td>Auth Status:</td><td>Status:</td><td>Error:</td><td>Org(ID):</td></tr><tr><td align='center' colspan='6'><i>Logs data is missing</i></td></tr></table><br><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Lookup(s) summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Count:</td><td>Endpoint:</td><td>Status:</td><td>Error:</td><td>Org(ID):</td></tr><tr><td align='center' colspan='5'><i>Logs data is missing</i></td></tr></table><br><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Request(s) summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Count:</td><td>Response code:</td><td>Endpoint:</td><td>Status:</td><td>Error:</td><td>Org(ID):</td></tr><tr><td bgcolor='#FFFFFF'>1440</td><td bgcolor='#FFFFFF'>None</td><td bgcolor='#FFFFFF'>util/version</td><td bgcolor='#FFFFFF'>success</td><td bgcolor='#FFFFFF'></td><td bgcolor='#FFFFFF'>OrgNotFound (util)</td></tr></table><br><table><tr><td bgcolor='#00A303' align='center' width='800'><font size='3' color='white'><b>STATUS - PASSED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;USER ACCOUNTS</b></font></td></tr></table><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>User Accounts Request(s) summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Count:</td><td>Email:</td><td>Respose Code:</td><td>Endpoint:</td><td>Status:</td><td>Error:</td></tr><tr><td bgcolor='#FFFF00'>1</td><td bgcolor='#FFFF00'>None</td><td bgcolor='#FFFF00'>500</td><td bgcolor='#FFFF00'>/customer/{customerId}</td><td bgcolor='#FFFF00'>error</td><td bgcolor='#FFFF00'>For input string: "00000000util"</td></tr><tr><td bgcolor='#FFFFFF'>701</td><td bgcolor='#FFFFFF'>None</td><td bgcolor='#FFFFFF'>200</td><td bgcolor='#FFFFFF'>/customer/{customerId}</td><td bgcolor='#FFFFFF'>success</td><td bgcolor='#FFFFFF'>None</td></tr><tr><td bgcolor='#FFFFFF'>31</td><td bgcolor='#FFFFFF'>system_qa@apixio.com</td><td bgcolor='#FFFFFF'>200</td><td bgcolor='#FFFFFF'>/auths</td><td bgcolor='#FFFFFF'>success</td><td bgcolor='#FFFFFF'>None</td></tr><tr><td bgcolor='#FFFFFF'>4</td><td bgcolor='#FFFFFF'>alarocca@apixio.com</td><td bgcolor='#FFFFFF'>200</td><td bgcolor='#FFFFFF'>/auths</td><td bgcolor='#FFFFFF'>success</td><td bgcolor='#FFFFFF'>None</td></tr></table><br><table><tr><td bgcolor='#DF1000' align='center' width='800'><font size='3' color='white'><b>STATUS - FAILED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;BUNDLER</b></font></td></tr></table><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Bundler Sequence summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Count:</td><td>Pattern:</td><td>Memory Total Bytes:</td><td>Status:</td></tr><tr><td align='center' colspan='4'><i>Logs data is missing</i></td></tr></table><br><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Bundler Historical summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Count:</td><td>Low:</td><td>High:</td><td>Status:</td><td>Millis:</td><td>Memory Total Bytes:</td></tr><tr><td align='center' colspan='6'><i>Logs data is missing</i></td></tr></table><br><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Bundler Document(s) summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Document count:</td><td>Org(ID):</td></tr><tr><td align='center' colspan='2'><i>Logs data is missing</i></td></tr></table><br><table><tr><td bgcolor='#00A303' align='center' width='800'><font size='3' color='white'><b>STATUS - PASSED</b></font></td></tr></table><br><br><table><tr><td bgcolor='#4E4E4E' align='left' width='800'><font size='3' color='white'><b>&nbsp;&nbsp;LOADER</b></font></td></tr></table><table border='0' cellpadding='1' cellspacing='0'><tr><td><b>Loader Document(s) summary</b></td></tr></table><table border='1' cellpadding='1' cellspacing='0' width='800'><tr><td>Document count:</td><td>Batch name:</td><td>User:</td><td>Success:</td><td>Attempts:</td><td>Org(ID):</td></tr><tr><td align='center' colspan='6'><i>Logs data is missing</i></td></tr></table><br><table><tr><td bgcolor='#00A303' align='center' width='800'><font size='3' color='white'><b>STATUS - PASSED</b></font></td></tr></table><br><br><table><tr><td><br>End of Daily engineering QA - 06/01/2015 17:13:09<br><br></td></tr><tr><td><br><i>-- Apixio QA Team</i></td></tr></table>